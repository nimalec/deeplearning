{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gByqIKs0yldw"
   },
   "source": [
    "\n",
    "# Homework 2, Problem 5 \n",
    "\n",
    "Contributor: Nima Leclerc (nleclerc@seas.upenn.edu) \n",
    "Date: October 13, 2020 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7dRXlLTymbL"
   },
   "outputs": [],
   "source": [
    "LOG_DIR = './logs'\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")\n",
    "\n",
    "!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7WfttUuyqr6"
   },
   "outputs": [],
   "source": [
    "get_ipython().system_raw('./ngrok http 6006 &')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jOwYYRjXytMw",
    "outputId": "2a69243e-4a72-4b3f-e8aa-d620dab36682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard Link: https://d498673af185.ngrok.io\n"
     ]
    }
   ],
   "source": [
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "id": "DTbo9if9VUen",
    "outputId": "a6d8554e-2853-4467-ac09-c5cadef50538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.15\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
      "\u001b[K     |████████████████████████████████| 412.3MB 41kB/s \n",
      "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 50.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.3.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.4)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 53.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.10.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.35.1)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n",
      "Collecting keras-applications>=1.0.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.32.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (50.3.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.0)\n",
      "Building wheels for collected packages: gast\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=de70c0ab808919e343f81ee8c72b898ed5ca514b7cab2c18861c790db09f1ec6\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "Successfully built gast\n",
      "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, gast, keras-applications, tensorflow\n",
      "  Found existing installation: tensorflow-estimator 2.3.0\n",
      "    Uninstalling tensorflow-estimator-2.3.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
      "  Found existing installation: tensorboard 2.3.0\n",
      "    Uninstalling tensorboard-2.3.0:\n",
      "      Successfully uninstalled tensorboard-2.3.0\n",
      "  Found existing installation: gast 0.3.3\n",
      "    Uninstalling gast-0.3.3:\n",
      "      Successfully uninstalled gast-0.3.3\n",
      "  Found existing installation: tensorflow 2.3.0\n",
      "    Uninstalling tensorflow-2.3.0:\n",
      "      Successfully uninstalled tensorflow-2.3.0\n",
      "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhY5ZXNlyu21"
   },
   "outputs": [],
   "source": [
    "# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc \n",
    "try:\n",
    "    from StringIO import StringIO  # Python 2.7\n",
    "except ImportError:\n",
    "    from io import BytesIO         # Python 3.x\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    def image_summary(self, tag, images, step):\n",
    "        \"\"\"Log a list of images.\"\"\"\n",
    "\n",
    "        img_summaries = []\n",
    "        for i, img in enumerate(images):\n",
    "            # Write the image to a string\n",
    "            try:\n",
    "                s = StringIO()\n",
    "            except:\n",
    "                s = BytesIO()\n",
    "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
    "\n",
    "            # Create an Image object\n",
    "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
    "                                       height=img.shape[0],\n",
    "                                       width=img.shape[1])\n",
    "            # Create a Summary value\n",
    "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=img_summaries)\n",
    "        self.writer.add_summary(summary, step)\n",
    "        \n",
    "    def histo_summary(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
    "\n",
    "        # Create a histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        # Fill the fields of the histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "\n",
    "        # Drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "2pFckC9wIgl_",
    "outputId": "ca8ebc90-8e02-4745-abab-12ce0048acc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Comment the following code if not on colab\n",
    "# bootstrap environment into place\n",
    "from google.colab import auth\n",
    "import datetime\n",
    "auth.authenticate_user()\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "drive_service = build('drive', 'v3')\n",
    "\n",
    "import io\n",
    "import os\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "# mount the user's google drive into the system\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gdQ33nZOy2Qx",
    "outputId": "91356c91-2b48-4d69-9d56-7794977e4874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# torch and torchvision imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "logger = Logger('./logs')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "print(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "aHrThjquy3xP",
    "outputId": "449a147f-d78f-42dc-bc6c-70ed3a0f283e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Reading in the dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "# Defining the model\n",
    "class View(nn.Module):\n",
    "    def __init__(self,o):\n",
    "        super().__init__()\n",
    "        self.o = o\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x.view(-1, self.o)\n",
    "    \n",
    "class allcnn_t(nn.Module):\n",
    "    def __init__(self, c1=96, c2= 192):\n",
    "        super().__init__()\n",
    "        d = 0.5\n",
    "\n",
    "        def convbn(ci,co,ksz,s=1,pz=0):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(ci,co,ksz,stride=s,padding=pz),\n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(co))\n",
    "\n",
    "        self.m = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            convbn(3,c1,3,1,1),\n",
    "            convbn(c1,c1,3,1,1),\n",
    "            convbn(c1,c1,3,2,1),\n",
    "            nn.Dropout(d),\n",
    "            convbn(c1,c2,3,1,1),\n",
    "            convbn(c2,c2,3,1,1),\n",
    "            convbn(c2,c2,3,2,1),\n",
    "            nn.Dropout(d),\n",
    "            convbn(c2,c2,3,1,1),\n",
    "            convbn(c2,c2,3,1,1),\n",
    "            convbn(c2,10,1,1),\n",
    "            nn.AvgPool2d(8),\n",
    "            View(10))\n",
    "\n",
    "        print('Num parameters: ', sum([p.numel() for p in self.m.parameters()]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.m(x)\n",
    "\n",
    "# The training loop\n",
    "\n",
    "def train(net, optimizer, criterion, train_loader, test_loader, epochs, model_name, plot):\n",
    "    model = net.to(device)\n",
    "    total_step = len(train_loader)\n",
    "    overall_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Move tensors to configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            #Forward Pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1) % 1000 == 0:\n",
    "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
    "            if plot:\n",
    "              info = { ('loss_' + model_name): loss.item() }\n",
    "\n",
    "              for tag, value in info.items():\n",
    "                logger.scalar_summary(tag, value, overall_step+1)\n",
    "\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ABZRNzZOzTuZ",
    "outputId": "8c641655-e572-4c43-9cdb-9ae08147cc49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters:  1667166\n"
     ]
    }
   ],
   "source": [
    "model = allcnn_t().to(device)\n",
    "epoch_1 = 40 \n",
    "epoch_2 = 40 \n",
    "epoch_3 = 20 \n",
    "alpha_1 = 0.1\n",
    "alpha_2 = 0.01 \n",
    "alpha_3 = 0.001 \n",
    "decay = 0.001 \n",
    "decay_final = 0.0001 \n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb6sqXLA6njH"
   },
   "source": [
    "(a) The model is trained on the CIFAR-10 dataset using a weight-decay of 0.001 and momentum of 0.9. The learning rate for the first 40 epochs is set to 0.1, for the second 40 epochs is 0.01, and 0.001 for the final 20 epochs. These will be staggered into 3 seperate rounds of training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9DblBkUrFlg"
   },
   "source": [
    "First round of training: alpha = 0.1, beta = 0.9, weight decay = 0.001 for 40 epochs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GPKoXPbR6Qbo",
    "outputId": "af2ffd2f-64f8-401a-8a4b-d51e3b9fd964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Step [1000/3125], Loss: 1.5895\n",
      "Epoch [1/40], Step [2000/3125], Loss: 1.5407\n",
      "Epoch [1/40], Step [3000/3125], Loss: 1.3922\n",
      "Epoch [2/40], Step [1000/3125], Loss: 1.3556\n",
      "Epoch [2/40], Step [2000/3125], Loss: 1.6914\n",
      "Epoch [2/40], Step [3000/3125], Loss: 1.4758\n",
      "Epoch [3/40], Step [1000/3125], Loss: 2.0685\n",
      "Epoch [3/40], Step [2000/3125], Loss: 1.8247\n",
      "Epoch [3/40], Step [3000/3125], Loss: 1.5538\n",
      "Epoch [4/40], Step [1000/3125], Loss: 1.2449\n",
      "Epoch [4/40], Step [2000/3125], Loss: 1.6770\n",
      "Epoch [4/40], Step [3000/3125], Loss: 1.4262\n",
      "Epoch [5/40], Step [1000/3125], Loss: 1.7119\n",
      "Epoch [5/40], Step [2000/3125], Loss: 1.2999\n",
      "Epoch [5/40], Step [3000/3125], Loss: 1.4009\n",
      "Epoch [6/40], Step [1000/3125], Loss: 1.7566\n",
      "Epoch [6/40], Step [2000/3125], Loss: 1.5592\n",
      "Epoch [6/40], Step [3000/3125], Loss: 1.0612\n",
      "Epoch [7/40], Step [1000/3125], Loss: 1.0676\n",
      "Epoch [7/40], Step [2000/3125], Loss: 1.6733\n",
      "Epoch [7/40], Step [3000/3125], Loss: 1.3834\n",
      "Epoch [8/40], Step [1000/3125], Loss: 1.4796\n",
      "Epoch [8/40], Step [2000/3125], Loss: 1.4662\n",
      "Epoch [8/40], Step [3000/3125], Loss: 1.6103\n",
      "Epoch [9/40], Step [1000/3125], Loss: 1.3113\n",
      "Epoch [9/40], Step [2000/3125], Loss: 1.6755\n",
      "Epoch [9/40], Step [3000/3125], Loss: 1.3735\n",
      "Epoch [10/40], Step [1000/3125], Loss: 1.4599\n",
      "Epoch [10/40], Step [2000/3125], Loss: 1.2812\n",
      "Epoch [10/40], Step [3000/3125], Loss: 1.5200\n",
      "Epoch [11/40], Step [1000/3125], Loss: 1.3485\n",
      "Epoch [11/40], Step [2000/3125], Loss: 1.6527\n",
      "Epoch [11/40], Step [3000/3125], Loss: 1.6292\n",
      "Epoch [12/40], Step [1000/3125], Loss: 1.5459\n",
      "Epoch [12/40], Step [2000/3125], Loss: 1.5349\n",
      "Epoch [12/40], Step [3000/3125], Loss: 1.4219\n",
      "Epoch [13/40], Step [1000/3125], Loss: 1.8505\n",
      "Epoch [13/40], Step [2000/3125], Loss: 1.5849\n",
      "Epoch [13/40], Step [3000/3125], Loss: 1.3989\n",
      "Epoch [14/40], Step [1000/3125], Loss: 1.1679\n",
      "Epoch [14/40], Step [2000/3125], Loss: 1.2906\n",
      "Epoch [14/40], Step [3000/3125], Loss: 2.0222\n",
      "Epoch [15/40], Step [1000/3125], Loss: 1.1992\n",
      "Epoch [15/40], Step [2000/3125], Loss: 1.6336\n",
      "Epoch [15/40], Step [3000/3125], Loss: 1.5650\n",
      "Epoch [16/40], Step [1000/3125], Loss: 1.2540\n",
      "Epoch [16/40], Step [2000/3125], Loss: 1.4470\n",
      "Epoch [16/40], Step [3000/3125], Loss: 0.8289\n",
      "Epoch [17/40], Step [1000/3125], Loss: 1.7886\n",
      "Epoch [17/40], Step [2000/3125], Loss: 1.2180\n",
      "Epoch [17/40], Step [3000/3125], Loss: 1.4052\n",
      "Epoch [18/40], Step [1000/3125], Loss: 1.5948\n",
      "Epoch [18/40], Step [2000/3125], Loss: 1.8197\n",
      "Epoch [18/40], Step [3000/3125], Loss: 1.5912\n",
      "Epoch [19/40], Step [1000/3125], Loss: 1.8703\n",
      "Epoch [19/40], Step [2000/3125], Loss: 1.2867\n",
      "Epoch [19/40], Step [3000/3125], Loss: 1.4326\n",
      "Epoch [20/40], Step [1000/3125], Loss: 1.3421\n",
      "Epoch [20/40], Step [2000/3125], Loss: 1.5285\n",
      "Epoch [20/40], Step [3000/3125], Loss: 1.4646\n",
      "Epoch [21/40], Step [1000/3125], Loss: 1.9692\n",
      "Epoch [21/40], Step [2000/3125], Loss: 1.4432\n",
      "Epoch [21/40], Step [3000/3125], Loss: 2.0255\n",
      "Epoch [22/40], Step [1000/3125], Loss: 1.6334\n",
      "Epoch [22/40], Step [2000/3125], Loss: 1.2228\n",
      "Epoch [22/40], Step [3000/3125], Loss: 1.0201\n",
      "Epoch [23/40], Step [1000/3125], Loss: 1.2566\n",
      "Epoch [23/40], Step [2000/3125], Loss: 1.3434\n",
      "Epoch [23/40], Step [3000/3125], Loss: 1.3954\n",
      "Epoch [24/40], Step [1000/3125], Loss: 1.3085\n",
      "Epoch [24/40], Step [2000/3125], Loss: 1.4664\n",
      "Epoch [24/40], Step [3000/3125], Loss: 1.4577\n",
      "Epoch [25/40], Step [1000/3125], Loss: 1.7990\n",
      "Epoch [25/40], Step [2000/3125], Loss: 1.5890\n",
      "Epoch [25/40], Step [3000/3125], Loss: 1.5527\n",
      "Epoch [26/40], Step [1000/3125], Loss: 1.6239\n",
      "Epoch [26/40], Step [2000/3125], Loss: 1.1837\n",
      "Epoch [26/40], Step [3000/3125], Loss: 1.2090\n",
      "Epoch [27/40], Step [1000/3125], Loss: 1.5498\n",
      "Epoch [27/40], Step [2000/3125], Loss: 1.6786\n",
      "Epoch [27/40], Step [3000/3125], Loss: 1.3650\n",
      "Epoch [28/40], Step [1000/3125], Loss: 1.2566\n",
      "Epoch [28/40], Step [2000/3125], Loss: 1.8261\n",
      "Epoch [28/40], Step [3000/3125], Loss: 1.3212\n",
      "Epoch [29/40], Step [1000/3125], Loss: 1.8317\n",
      "Epoch [29/40], Step [2000/3125], Loss: 1.6291\n",
      "Epoch [29/40], Step [3000/3125], Loss: 1.1790\n",
      "Epoch [30/40], Step [1000/3125], Loss: 1.5781\n",
      "Epoch [30/40], Step [2000/3125], Loss: 1.4702\n",
      "Epoch [30/40], Step [3000/3125], Loss: 1.5055\n",
      "Epoch [31/40], Step [1000/3125], Loss: 1.4228\n",
      "Epoch [31/40], Step [2000/3125], Loss: 1.1744\n",
      "Epoch [31/40], Step [3000/3125], Loss: 1.3432\n",
      "Epoch [32/40], Step [1000/3125], Loss: 1.5016\n",
      "Epoch [32/40], Step [2000/3125], Loss: 1.5323\n",
      "Epoch [32/40], Step [3000/3125], Loss: 1.0122\n",
      "Epoch [33/40], Step [1000/3125], Loss: 1.7811\n",
      "Epoch [33/40], Step [2000/3125], Loss: 1.2287\n",
      "Epoch [33/40], Step [3000/3125], Loss: 1.5014\n",
      "Epoch [34/40], Step [1000/3125], Loss: 1.5585\n",
      "Epoch [34/40], Step [2000/3125], Loss: 2.0391\n",
      "Epoch [34/40], Step [3000/3125], Loss: 1.1078\n",
      "Epoch [35/40], Step [1000/3125], Loss: 1.4601\n",
      "Epoch [35/40], Step [2000/3125], Loss: 1.5290\n",
      "Epoch [35/40], Step [3000/3125], Loss: 1.7232\n",
      "Epoch [36/40], Step [1000/3125], Loss: 1.6871\n",
      "Epoch [36/40], Step [2000/3125], Loss: 1.2777\n",
      "Epoch [36/40], Step [3000/3125], Loss: 1.3348\n",
      "Epoch [37/40], Step [1000/3125], Loss: 1.7140\n",
      "Epoch [37/40], Step [2000/3125], Loss: 1.4913\n",
      "Epoch [37/40], Step [3000/3125], Loss: 1.3283\n",
      "Epoch [38/40], Step [1000/3125], Loss: 1.0844\n",
      "Epoch [38/40], Step [2000/3125], Loss: 1.8268\n",
      "Epoch [38/40], Step [3000/3125], Loss: 1.8047\n",
      "Epoch [39/40], Step [1000/3125], Loss: 1.4585\n",
      "Epoch [39/40], Step [2000/3125], Loss: 1.1868\n",
      "Epoch [39/40], Step [3000/3125], Loss: 1.6290\n",
      "Epoch [40/40], Step [1000/3125], Loss: 1.1752\n",
      "Epoch [40/40], Step [2000/3125], Loss: 1.4158\n",
      "Epoch [40/40], Step [3000/3125], Loss: 1.5190\n",
      "Accuracy of the network on the test images: 46.32 %\n"
     ]
    }
   ],
   "source": [
    "optimizer1 = optim.SGD(model.parameters(), lr=alpha_1, momentum=0.9, weight_decay=0.001)\n",
    "train(model, optimizer1, criterion, trainloader, testloader, epoch_1, 'cnn', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bO-um3oErYed"
   },
   "source": [
    "A test accuracy of 45.98 % in the first 40 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhMIwdE2i73_"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/content/allcnn_v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1J0GE6Or0al"
   },
   "source": [
    "Second round of training: alpha = 0.01, beta = 0.9, weight decay = 0.001 for 40 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "26TzJ1Wj7yzd",
    "outputId": "ce9313df-1a86-46d8-d2e0-67775f3f08f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Step [1000/3125], Loss: 0.8973\n",
      "Epoch [1/40], Step [2000/3125], Loss: 0.7319\n",
      "Epoch [1/40], Step [3000/3125], Loss: 1.3257\n",
      "Epoch [2/40], Step [1000/3125], Loss: 0.9179\n",
      "Epoch [2/40], Step [2000/3125], Loss: 0.9999\n",
      "Epoch [2/40], Step [3000/3125], Loss: 0.7893\n",
      "Epoch [3/40], Step [1000/3125], Loss: 0.6011\n",
      "Epoch [3/40], Step [2000/3125], Loss: 0.9540\n",
      "Epoch [3/40], Step [3000/3125], Loss: 0.6341\n",
      "Epoch [4/40], Step [1000/3125], Loss: 0.3015\n",
      "Epoch [4/40], Step [2000/3125], Loss: 0.6082\n",
      "Epoch [4/40], Step [3000/3125], Loss: 0.8472\n",
      "Epoch [5/40], Step [1000/3125], Loss: 0.7760\n",
      "Epoch [5/40], Step [2000/3125], Loss: 0.3909\n",
      "Epoch [5/40], Step [3000/3125], Loss: 0.8702\n",
      "Epoch [6/40], Step [1000/3125], Loss: 0.3182\n",
      "Epoch [6/40], Step [2000/3125], Loss: 0.4683\n",
      "Epoch [6/40], Step [3000/3125], Loss: 1.0927\n",
      "Epoch [7/40], Step [1000/3125], Loss: 0.6552\n",
      "Epoch [7/40], Step [2000/3125], Loss: 0.7552\n",
      "Epoch [7/40], Step [3000/3125], Loss: 0.5908\n",
      "Epoch [8/40], Step [1000/3125], Loss: 0.7532\n",
      "Epoch [8/40], Step [2000/3125], Loss: 0.7591\n",
      "Epoch [8/40], Step [3000/3125], Loss: 0.2960\n",
      "Epoch [9/40], Step [1000/3125], Loss: 0.9000\n",
      "Epoch [9/40], Step [2000/3125], Loss: 0.7771\n",
      "Epoch [9/40], Step [3000/3125], Loss: 1.1401\n",
      "Epoch [10/40], Step [1000/3125], Loss: 0.2867\n",
      "Epoch [10/40], Step [2000/3125], Loss: 0.8024\n",
      "Epoch [10/40], Step [3000/3125], Loss: 0.7923\n",
      "Epoch [11/40], Step [1000/3125], Loss: 0.5678\n",
      "Epoch [11/40], Step [2000/3125], Loss: 0.6916\n",
      "Epoch [11/40], Step [3000/3125], Loss: 0.3926\n",
      "Epoch [12/40], Step [1000/3125], Loss: 0.6868\n",
      "Epoch [12/40], Step [2000/3125], Loss: 0.6717\n",
      "Epoch [12/40], Step [3000/3125], Loss: 0.5339\n",
      "Epoch [13/40], Step [1000/3125], Loss: 0.3143\n",
      "Epoch [13/40], Step [2000/3125], Loss: 0.5246\n",
      "Epoch [13/40], Step [3000/3125], Loss: 0.3735\n",
      "Epoch [14/40], Step [1000/3125], Loss: 0.6681\n",
      "Epoch [14/40], Step [2000/3125], Loss: 0.5356\n",
      "Epoch [14/40], Step [3000/3125], Loss: 0.4508\n",
      "Epoch [15/40], Step [1000/3125], Loss: 0.4868\n",
      "Epoch [15/40], Step [2000/3125], Loss: 0.7334\n",
      "Epoch [15/40], Step [3000/3125], Loss: 0.2648\n",
      "Epoch [16/40], Step [1000/3125], Loss: 0.4600\n",
      "Epoch [16/40], Step [2000/3125], Loss: 0.6268\n",
      "Epoch [16/40], Step [3000/3125], Loss: 0.2567\n",
      "Epoch [17/40], Step [1000/3125], Loss: 0.1553\n",
      "Epoch [17/40], Step [2000/3125], Loss: 0.4064\n",
      "Epoch [17/40], Step [3000/3125], Loss: 0.5562\n",
      "Epoch [18/40], Step [1000/3125], Loss: 0.8738\n",
      "Epoch [18/40], Step [2000/3125], Loss: 0.3369\n",
      "Epoch [18/40], Step [3000/3125], Loss: 0.3040\n",
      "Epoch [19/40], Step [1000/3125], Loss: 0.6002\n",
      "Epoch [19/40], Step [2000/3125], Loss: 0.2737\n",
      "Epoch [19/40], Step [3000/3125], Loss: 0.4205\n",
      "Epoch [20/40], Step [1000/3125], Loss: 0.3569\n",
      "Epoch [20/40], Step [2000/3125], Loss: 0.9233\n",
      "Epoch [20/40], Step [3000/3125], Loss: 0.3483\n",
      "Epoch [21/40], Step [1000/3125], Loss: 0.6975\n",
      "Epoch [21/40], Step [2000/3125], Loss: 0.4245\n",
      "Epoch [21/40], Step [3000/3125], Loss: 0.6015\n",
      "Epoch [22/40], Step [1000/3125], Loss: 0.3424\n",
      "Epoch [22/40], Step [2000/3125], Loss: 0.5215\n",
      "Epoch [22/40], Step [3000/3125], Loss: 0.4561\n",
      "Epoch [23/40], Step [1000/3125], Loss: 0.3370\n",
      "Epoch [23/40], Step [2000/3125], Loss: 0.8624\n",
      "Epoch [23/40], Step [3000/3125], Loss: 0.6120\n",
      "Epoch [24/40], Step [1000/3125], Loss: 0.3290\n",
      "Epoch [24/40], Step [2000/3125], Loss: 1.9028\n",
      "Epoch [24/40], Step [3000/3125], Loss: 0.5082\n",
      "Epoch [25/40], Step [1000/3125], Loss: 0.5778\n",
      "Epoch [25/40], Step [2000/3125], Loss: 0.3739\n",
      "Epoch [25/40], Step [3000/3125], Loss: 0.5605\n",
      "Epoch [26/40], Step [1000/3125], Loss: 0.4082\n",
      "Epoch [26/40], Step [2000/3125], Loss: 0.2900\n",
      "Epoch [26/40], Step [3000/3125], Loss: 0.3589\n",
      "Epoch [27/40], Step [1000/3125], Loss: 0.2422\n",
      "Epoch [27/40], Step [2000/3125], Loss: 0.2358\n",
      "Epoch [27/40], Step [3000/3125], Loss: 0.1678\n",
      "Epoch [28/40], Step [1000/3125], Loss: 0.5366\n",
      "Epoch [28/40], Step [2000/3125], Loss: 0.4331\n",
      "Epoch [28/40], Step [3000/3125], Loss: 0.1782\n",
      "Epoch [29/40], Step [1000/3125], Loss: 0.3617\n",
      "Epoch [29/40], Step [2000/3125], Loss: 0.6956\n",
      "Epoch [29/40], Step [3000/3125], Loss: 1.0471\n",
      "Epoch [30/40], Step [1000/3125], Loss: 0.9697\n",
      "Epoch [30/40], Step [2000/3125], Loss: 0.0332\n",
      "Epoch [30/40], Step [3000/3125], Loss: 0.1872\n",
      "Epoch [31/40], Step [1000/3125], Loss: 0.2816\n",
      "Epoch [31/40], Step [2000/3125], Loss: 0.4556\n",
      "Epoch [31/40], Step [3000/3125], Loss: 0.4855\n",
      "Epoch [32/40], Step [1000/3125], Loss: 0.7154\n",
      "Epoch [32/40], Step [2000/3125], Loss: 0.5701\n",
      "Epoch [32/40], Step [3000/3125], Loss: 0.9258\n",
      "Epoch [33/40], Step [1000/3125], Loss: 0.6346\n",
      "Epoch [33/40], Step [2000/3125], Loss: 0.4020\n",
      "Epoch [33/40], Step [3000/3125], Loss: 0.3727\n",
      "Epoch [34/40], Step [1000/3125], Loss: 0.5124\n",
      "Epoch [34/40], Step [2000/3125], Loss: 0.5521\n",
      "Epoch [34/40], Step [3000/3125], Loss: 0.6716\n",
      "Epoch [35/40], Step [1000/3125], Loss: 0.5246\n",
      "Epoch [35/40], Step [2000/3125], Loss: 0.5185\n",
      "Epoch [35/40], Step [3000/3125], Loss: 0.7742\n",
      "Epoch [36/40], Step [1000/3125], Loss: 0.4050\n",
      "Epoch [36/40], Step [2000/3125], Loss: 0.4337\n",
      "Epoch [36/40], Step [3000/3125], Loss: 0.7693\n",
      "Epoch [37/40], Step [1000/3125], Loss: 0.3935\n",
      "Epoch [37/40], Step [2000/3125], Loss: 0.1931\n",
      "Epoch [37/40], Step [3000/3125], Loss: 0.2753\n",
      "Epoch [38/40], Step [1000/3125], Loss: 0.4350\n",
      "Epoch [38/40], Step [2000/3125], Loss: 0.5988\n",
      "Epoch [38/40], Step [3000/3125], Loss: 0.3274\n",
      "Epoch [39/40], Step [1000/3125], Loss: 0.1919\n",
      "Epoch [39/40], Step [2000/3125], Loss: 0.3781\n",
      "Epoch [39/40], Step [3000/3125], Loss: 0.4495\n",
      "Epoch [40/40], Step [1000/3125], Loss: 0.3081\n",
      "Epoch [40/40], Step [2000/3125], Loss: 0.1885\n",
      "Epoch [40/40], Step [3000/3125], Loss: 0.0871\n",
      "Accuracy of the network on the test images: 81.24 %\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = optim.SGD(model.parameters(), lr=alpha_2, momentum=0.9, weight_decay=0.001)\n",
    "train(model, optimizer2, criterion, trainloader, testloader, epoch_2, 'cnn', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ovjqjmssRot"
   },
   "source": [
    "Second round of training yields a test accuracy of 81.24 % after a total of 80 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyH2oYCbbwIM"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/content/allcnn_v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZE9PcijRr69E"
   },
   "source": [
    "Third round of training: alpha = 0.001, beta = 0.9, weight decay = 0.001 for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "reLKAjEs8JXT",
    "outputId": "fae59def-6768-4bc1-9f94-b2e807105300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [1000/3125], Loss: 0.0706\n",
      "Epoch [1/20], Step [2000/3125], Loss: 0.3540\n",
      "Epoch [1/20], Step [3000/3125], Loss: 0.0744\n",
      "Epoch [2/20], Step [1000/3125], Loss: 0.0288\n",
      "Epoch [2/20], Step [2000/3125], Loss: 0.0456\n",
      "Epoch [2/20], Step [3000/3125], Loss: 0.2865\n",
      "Epoch [3/20], Step [1000/3125], Loss: 0.0745\n",
      "Epoch [3/20], Step [2000/3125], Loss: 0.1764\n",
      "Epoch [3/20], Step [3000/3125], Loss: 0.0383\n",
      "Epoch [4/20], Step [1000/3125], Loss: 0.0484\n",
      "Epoch [4/20], Step [2000/3125], Loss: 0.1845\n",
      "Epoch [4/20], Step [3000/3125], Loss: 0.0880\n",
      "Epoch [5/20], Step [1000/3125], Loss: 0.1191\n",
      "Epoch [5/20], Step [2000/3125], Loss: 0.0625\n",
      "Epoch [5/20], Step [3000/3125], Loss: 0.0511\n",
      "Epoch [6/20], Step [1000/3125], Loss: 0.0272\n",
      "Epoch [6/20], Step [2000/3125], Loss: 0.0120\n",
      "Epoch [6/20], Step [3000/3125], Loss: 0.0143\n",
      "Epoch [7/20], Step [1000/3125], Loss: 0.0068\n",
      "Epoch [7/20], Step [2000/3125], Loss: 0.0653\n",
      "Epoch [7/20], Step [3000/3125], Loss: 0.0995\n",
      "Epoch [8/20], Step [1000/3125], Loss: 0.0100\n",
      "Epoch [8/20], Step [2000/3125], Loss: 0.0120\n",
      "Epoch [8/20], Step [3000/3125], Loss: 0.0166\n",
      "Epoch [9/20], Step [1000/3125], Loss: 0.0122\n",
      "Epoch [9/20], Step [2000/3125], Loss: 0.0091\n",
      "Epoch [9/20], Step [3000/3125], Loss: 0.0591\n",
      "Epoch [10/20], Step [1000/3125], Loss: 0.0128\n",
      "Epoch [10/20], Step [2000/3125], Loss: 0.0287\n",
      "Epoch [10/20], Step [3000/3125], Loss: 0.0130\n",
      "Epoch [11/20], Step [1000/3125], Loss: 0.0167\n",
      "Epoch [11/20], Step [2000/3125], Loss: 0.0246\n",
      "Epoch [11/20], Step [3000/3125], Loss: 0.0335\n",
      "Epoch [12/20], Step [1000/3125], Loss: 0.0080\n",
      "Epoch [12/20], Step [2000/3125], Loss: 0.0481\n",
      "Epoch [12/20], Step [3000/3125], Loss: 0.0099\n",
      "Epoch [13/20], Step [1000/3125], Loss: 0.0233\n",
      "Epoch [13/20], Step [2000/3125], Loss: 0.0394\n",
      "Epoch [13/20], Step [3000/3125], Loss: 0.0303\n",
      "Epoch [14/20], Step [1000/3125], Loss: 0.0250\n",
      "Epoch [14/20], Step [2000/3125], Loss: 0.0209\n",
      "Epoch [14/20], Step [3000/3125], Loss: 0.0108\n",
      "Epoch [15/20], Step [1000/3125], Loss: 0.0182\n",
      "Epoch [15/20], Step [2000/3125], Loss: 0.0023\n",
      "Epoch [15/20], Step [3000/3125], Loss: 0.0018\n",
      "Epoch [16/20], Step [1000/3125], Loss: 0.0239\n",
      "Epoch [16/20], Step [2000/3125], Loss: 0.0404\n",
      "Epoch [16/20], Step [3000/3125], Loss: 0.0408\n",
      "Epoch [17/20], Step [1000/3125], Loss: 0.0162\n",
      "Epoch [17/20], Step [2000/3125], Loss: 0.0235\n",
      "Epoch [17/20], Step [3000/3125], Loss: 0.0200\n",
      "Epoch [18/20], Step [1000/3125], Loss: 0.0169\n",
      "Epoch [18/20], Step [2000/3125], Loss: 0.0268\n",
      "Epoch [18/20], Step [3000/3125], Loss: 0.1172\n",
      "Epoch [19/20], Step [1000/3125], Loss: 0.0014\n",
      "Epoch [19/20], Step [2000/3125], Loss: 0.0059\n",
      "Epoch [19/20], Step [3000/3125], Loss: 0.0282\n",
      "Epoch [20/20], Step [1000/3125], Loss: 0.0253\n",
      "Epoch [20/20], Step [2000/3125], Loss: 0.0126\n",
      "Epoch [20/20], Step [3000/3125], Loss: 0.1391\n",
      "Accuracy of the network on the test images: 83.18 %\n"
     ]
    }
   ],
   "source": [
    "optimizer3 = optim.SGD(model.parameters(), lr=alpha_3, momentum=0.9, weight_decay=0.001)\n",
    "train(model, optimizer3, criterion, trainloader, testloader, epoch_3, 'cnn', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-LAXbB_sd8B"
   },
   "source": [
    "Third round of training yields a test accuracy of 83.18 % after a total of 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9KJC6pibxXC"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/content/allcnn_v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phVSuqgtsJGH"
   },
   "source": [
    "Fourth round of training: alpha = 0.0005, beta = 0.9, weight decay = 0.001 for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m_J4eR7AtcN4",
    "outputId": "2043e13c-ad6d-42ff-d981-005251ede608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [1000/3125], Loss: 0.0042\n",
      "Epoch [1/20], Step [2000/3125], Loss: 0.0044\n",
      "Epoch [1/20], Step [3000/3125], Loss: 0.0017\n",
      "Epoch [2/20], Step [1000/3125], Loss: 0.0039\n",
      "Epoch [2/20], Step [2000/3125], Loss: 0.0084\n",
      "Epoch [2/20], Step [3000/3125], Loss: 0.0097\n",
      "Epoch [3/20], Step [1000/3125], Loss: 0.0341\n",
      "Epoch [3/20], Step [2000/3125], Loss: 0.0041\n",
      "Epoch [3/20], Step [3000/3125], Loss: 0.0153\n",
      "Epoch [4/20], Step [1000/3125], Loss: 0.0107\n",
      "Epoch [4/20], Step [2000/3125], Loss: 0.0004\n",
      "Epoch [4/20], Step [3000/3125], Loss: 0.0029\n",
      "Epoch [5/20], Step [1000/3125], Loss: 0.0033\n",
      "Epoch [5/20], Step [2000/3125], Loss: 0.0022\n",
      "Epoch [5/20], Step [3000/3125], Loss: 0.0087\n",
      "Epoch [6/20], Step [1000/3125], Loss: 0.0077\n",
      "Epoch [6/20], Step [2000/3125], Loss: 0.0050\n",
      "Epoch [6/20], Step [3000/3125], Loss: 0.0072\n",
      "Epoch [7/20], Step [1000/3125], Loss: 0.0099\n",
      "Epoch [7/20], Step [2000/3125], Loss: 0.0064\n",
      "Epoch [7/20], Step [3000/3125], Loss: 0.0020\n",
      "Epoch [8/20], Step [1000/3125], Loss: 0.0030\n",
      "Epoch [8/20], Step [2000/3125], Loss: 0.0302\n",
      "Epoch [8/20], Step [3000/3125], Loss: 0.0165\n",
      "Epoch [9/20], Step [1000/3125], Loss: 0.0175\n",
      "Epoch [9/20], Step [2000/3125], Loss: 0.0069\n",
      "Epoch [9/20], Step [3000/3125], Loss: 0.0046\n",
      "Epoch [10/20], Step [1000/3125], Loss: 0.0019\n",
      "Epoch [10/20], Step [2000/3125], Loss: 0.0112\n",
      "Epoch [10/20], Step [3000/3125], Loss: 0.0103\n",
      "Epoch [11/20], Step [1000/3125], Loss: 0.1304\n",
      "Epoch [11/20], Step [2000/3125], Loss: 0.0014\n",
      "Epoch [11/20], Step [3000/3125], Loss: 0.0672\n",
      "Epoch [12/20], Step [1000/3125], Loss: 0.0116\n",
      "Epoch [12/20], Step [2000/3125], Loss: 0.0104\n",
      "Epoch [12/20], Step [3000/3125], Loss: 0.0112\n",
      "Epoch [13/20], Step [1000/3125], Loss: 0.0178\n",
      "Epoch [13/20], Step [2000/3125], Loss: 0.0070\n",
      "Epoch [13/20], Step [3000/3125], Loss: 0.0019\n",
      "Epoch [14/20], Step [1000/3125], Loss: 0.0010\n",
      "Epoch [14/20], Step [2000/3125], Loss: 0.0025\n",
      "Epoch [14/20], Step [3000/3125], Loss: 0.0152\n",
      "Epoch [15/20], Step [1000/3125], Loss: 0.0079\n",
      "Epoch [15/20], Step [2000/3125], Loss: 0.0109\n",
      "Epoch [15/20], Step [3000/3125], Loss: 0.0116\n",
      "Epoch [16/20], Step [1000/3125], Loss: 0.0090\n",
      "Epoch [16/20], Step [2000/3125], Loss: 0.0061\n",
      "Epoch [16/20], Step [3000/3125], Loss: 0.0844\n",
      "Epoch [17/20], Step [1000/3125], Loss: 0.0015\n",
      "Epoch [17/20], Step [2000/3125], Loss: 0.0058\n",
      "Epoch [17/20], Step [3000/3125], Loss: 0.0024\n",
      "Epoch [18/20], Step [1000/3125], Loss: 0.0070\n",
      "Epoch [18/20], Step [2000/3125], Loss: 0.0013\n",
      "Epoch [18/20], Step [3000/3125], Loss: 0.0411\n",
      "Epoch [19/20], Step [1000/3125], Loss: 0.0717\n",
      "Epoch [19/20], Step [2000/3125], Loss: 0.0033\n",
      "Epoch [19/20], Step [3000/3125], Loss: 0.0215\n",
      "Epoch [20/20], Step [1000/3125], Loss: 0.0386\n",
      "Epoch [20/20], Step [2000/3125], Loss: 0.0031\n",
      "Epoch [20/20], Step [3000/3125], Loss: 0.0084\n",
      "Accuracy of the network on the test images: 84.09 %\n"
     ]
    }
   ],
   "source": [
    "optimizer4 = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9, weight_decay=0.001)\n",
    "train(model, optimizer4, criterion, trainloader, testloader, epoch_3, 'cnn', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmH8J5BisjdB"
   },
   "source": [
    "Second round of training yields a test accuracy of 84.09% after a total of 120 epochs\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMyLjCQ9UfLY"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/content/allcnn_v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHlUqJ5sGh7K"
   },
   "source": [
    "Plotted below are the training losses, validation losses, and errors as a function of number of epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEgVDSQrG5ct"
   },
   "source": [
    "(b) We can now get the gradient dx for a few input images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "RTdBx9SSG3on",
    "outputId": "06bc120a-d6c7-48f9-9520-a35e3f648ab8"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-92378a608219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#yh.backward(images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#dx = x[0:].grad.data.clone()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     32\u001b[0m                                    \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" and output[\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                                    \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"] has a shape of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                                    + str(out.shape) + \".\")\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 raise RuntimeError(\"For complex Tensors, both grad_output and output\"\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Mismatch in shape: grad_output[0] has a shape of torch.Size([16, 3, 32, 32]) and output[0] has a shape of torch.Size([])."
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "for i, (images, labels) in enumerate(testloader):\n",
    "    # Move tensors to configured device\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    x = Variable(images, requires_grad=True)\n",
    "criterion = nn.CrossEntropyLoss()   \n",
    "yh = model(x)\n",
    "loss = criterion(yh,labels)\n",
    "y_test =  Variable(loss, requires_grad=True)\n",
    "y_test.backward(images)\n",
    "#yh.backward(images)\n",
    "#dx = x[0:].grad.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "lsyXEUWeYtY0",
    "outputId": "2bacccaa-0966-49f0-d67c-f2b1da329fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.ones(10), requires_grad=True)\n",
    "y = x * Variable(torch.linspace(1, 10, 10), requires_grad=False)\n",
    "y.backward(torch.ones(10))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZuNSRZdcqLc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "leclerc_nima_hw2_prob5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
